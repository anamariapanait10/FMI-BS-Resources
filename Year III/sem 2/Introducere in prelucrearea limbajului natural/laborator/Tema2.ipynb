{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 1 - Scrie o funcție care împarte un text în tokeni folosind regex."
      ],
      "metadata": {
        "id": "QQCoIW-1Ktp1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPNmSaczKlkO",
        "outputId": "2a0c9832-f476-4330-9e2c-534790b26c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'don', 't', 'like', 'to', 'keep', 'our', 'lovely', 'customers', 'waiting', 'for', 'long', 'We', 'hope', 'you', 'enjoy', 'Happy', 'Friday', 'LWWF', '', 'https', 't', 'co', 'smyYriipxI']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "\n",
        "def tokenize_text(text):\n",
        "    delimiters = r\"\\s|\\W+\"\n",
        "    tokens = re.split(delimiters, text)\n",
        "    return tokens\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "text = tweets[6]\n",
        "tokens = tokenize_text(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 2 - Scrie o funcție care înlocuiește toate emoticoanele (nu doar cele date ca exemplu) și emojiurile din text."
      ],
      "metadata": {
        "id": "yUACpGz7LDwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "import re\n",
        "import nltk\n",
        "import emoji\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "\n",
        "def replace_emoticons_and_emojis(text):\n",
        "  emoticons = {\n",
        "    \"lol\": r\":\\)\\)\\)\\)+\",\n",
        "    \"happy\": r\":[\\)|D+]\",\n",
        "    \"laugh\": r\":\\)\\)+\",\n",
        "    \"very_sad\": r\":\\(\\(\\(\\(+\",\n",
        "    \"sad\": r\":\\(+\",\n",
        "  }\n",
        "\n",
        "  for emoticon, regex in emoticons.items():\n",
        "    text = re.sub(regex, f\" :{emoticon}: \", text)\n",
        "\n",
        "  emoji.demojize(text)\n",
        "\n",
        "  return text\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "replaced_text = replace_emoticons_and_emojis(tweets[24])\n",
        "print(replaced_text)"
      ],
      "metadata": {
        "id": "C7j4hHQfKtO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6ee084-12e5-4e19-cfb8-64efa8c127f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.10.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💅🏽💋 -  :lol:  haven't seen you in years\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 3 - Scrie o funcție care primește un text și returnează varianta preprocesată. Funcția va converti toate numerele în cuvinte folosind num2words, va elimina linkurile, hashtagurile, mentions, punctuația, stopwords, va face toate textele literă mică și va aplica lematizarea sau stemming."
      ],
      "metadata": {
        "id": "psMFmiLlLTlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words\n",
        "import re\n",
        "from num2words import num2words\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convertim numerele în cuvinte\n",
        "    text = re.sub(r'\\b\\d+\\b', lambda x: num2words(x.group()), text)\n",
        "    # Eliminăm linkurile, mentions și hashtagurile\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+|@[^\\s]+|#[^\\s]+', '', text)\n",
        "    # Convertim textul în litere mici\n",
        "    text = text.lower()\n",
        "    # Eliminăm punctuația\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Tokenizăm textul\n",
        "    words = word_tokenize(text)\n",
        "    # Eliminăm stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Aplicăm lematizarea\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "    preprocessed_text = ' '.join(lemmatized_words)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "preprocessed_text = preprocess_text(tweets[6])\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "id": "qRVbMMX-L792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530b2928-7cd3-453f-f47d-9339005b7eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.13)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dont like keep lovely customer waiting long hope enjoy happy friday lwwf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 4 - Analizează setul de date. Uită-te la elemente de preprocesare sau alte features folosind ce ați învățat în primul laborator. Ce pare important?"
      ],
      "metadata": {
        "id": "2zH1qrCqLYCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "\n",
        "# Afișăm primele 10 tweet-uri pentru a observa caracteristicile\n",
        "for tweet in tweets[:10]:\n",
        "    print(tweet)"
      ],
      "metadata": {
        "id": "mzK52k2bL7I2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16606be-361f-48db-bc83-d8b6ba1be14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
            "@97sides CONGRATS :)\n",
            "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
            "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
            "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
            "We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\n",
            "@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.\n",
            "Jgh , but we have to go to Bayan :D bye\n",
            "As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\n",
            "\n",
            "Well… as the name implies :p.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizand primele 10 tweet-uri, putem observa urmatoarele:\n",
        "\n",
        "1. Prezența emoticoanelor pozitive (:), (:D): Emoticoanele sunt utilizate frecvent în tweet-uri pentru a exprima sentimente pozitive. Acestea sunt indicatori importanți ai sentimentului și ar trebui păstrate în text pentru analiza sentimentelor.\n",
        "\n",
        "2. Mențiuni și taguri (@, #): Multe tweet-uri conțin mențiuni ale altor utilizatori (@) și hashtaguri (#). Deși mențiunile adesea nu contribuie la sentimentul general al tweet-ului și ar putea fi eliminate în preprocesare, hashtagurile pot fi legate de subiectele discutate și ar putea fi păstrate sau tratate separat în funcție de scopul analizei.\n",
        "\n",
        "3. Linkuri: Câteva tweet-uri includ linkuri URL. Acestea ar putea fi eliminate deoarece nu contribuie la analiza textului.\n",
        "\n",
        "4. Textul informal și abrevierile: Tweet-urile conțin limbaj informal, abrevieri (de exemplu, \"Jgh\" - Just got home), și expresii specifice rețelelor sociale. Normalizarea acestor expresii ar putea fi necesară, în funcție de analiza dorită.\n",
        "\n",
        "5. Mențiuni despre evenimente sau acțiuni specifice: Unele tweet-uri menționează evenimente specifice (de exemplu, verificarea contului pe Facebook) sau acțiuni (de exemplu, apelarea unui centru de contact). Aceste detalii pot fi relevante pentru context, dar s-ar putea să nu contribuie direct la sentimentul general al textului.\n",
        "\n",
        "6. Limbaj pozitiv și entuziasm: Multe dintre tweet-uri exprimă sentimente pozitive prin cuvinte ca \"CONGRATS\", \"Happy Friday\", sau \"amazing track\". Păstrarea acestor expresii este utilă pentru analiza sentimentelor.\n",
        "\n",
        "Pentru preprocesare, pe baza observațiilor de mai sus, o să păstrez următoarele preprocesări:\n",
        "\n",
        "- Păstrarea emoticoanelor\n",
        "- Eliminarea linkurilor\n",
        "- Tratarea separată a hashtagurilor\n",
        "- Eliminarea mențiunilor (@)\n",
        "- Normalizarea limbajului informal și a abrevierilor\n"
      ],
      "metadata": {
        "id": "uOQ5VoUEl153"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 5 - Pe baza analizei anterioare scrie o funcție de preprocesare care elimină doar informația irelevantă. Poți generaliza funcția originală specificând în lista de parametri ce modificări vrei să faci la apelarea funcției. Alternativ, poți face o clasă care include o serie de funcții care pot fi alese la inițializare. Cu cât e mai general cu atât mai bine."
      ],
      "metadata": {
        "id": "mhg8-xEBLfyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self, remove_links=True, remove_mentions=True, remove_hashtags=True,\n",
        "                 lowercase=True, remove_punctuation=True, remove_stopwords=False,\n",
        "                 lemmatize=False, language='english'):\n",
        "        self.remove_links = remove_links\n",
        "        self.remove_mentions = remove_mentions\n",
        "        self.remove_hashtags = remove_hashtags\n",
        "        self.lowercase = lowercase\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "        self.lemmatize = lemmatize\n",
        "        self.language = language\n",
        "        if remove_stopwords or lemmatize:\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('wordnet')\n",
        "            nltk.download('punkt')\n",
        "            self.stop_words = set(stopwords.words(language))\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        if self.remove_links:\n",
        "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "        if self.remove_mentions:\n",
        "            text = re.sub(r'@\\w+', '', text)\n",
        "        if self.remove_hashtags:\n",
        "            text = re.sub(r'#\\w+', '', text)\n",
        "        if self.lowercase:\n",
        "            text = text.lower()\n",
        "        if self.remove_punctuation:\n",
        "            text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        if self.remove_stopwords or self.lemmatize:\n",
        "            words = word_tokenize(text)\n",
        "            if self.remove_stopwords:\n",
        "                words = [word for word in words if word not in self.stop_words]\n",
        "            if self.lemmatize:\n",
        "                words = [self.lemmatizer.lemmatize(word) for word in words]\n",
        "            text = ' '.join(words)\n",
        "        return text\n",
        "\n",
        "preprocessor = TextPreprocessor(remove_links=True, remove_mentions=True, remove_hashtags=False,\n",
        "                                lowercase=True, remove_punctuation=True, remove_stopwords=True,\n",
        "                                lemmatize=True)\n",
        "\n",
        "\n",
        "text = tweets[6]\n",
        "preprocessed_text = preprocessor.preprocess(text)\n",
        "print(\"-\"*40)\n",
        "print(preprocessed_text)\n",
        "print(\"-\"*40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv4bcMkeluSl",
        "outputId": "d3c3f01d-9588-4c39-b260-897f9300fcdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "dont like keep lovely customer waiting long hope enjoy happy friday lwwf\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 6 - Compară metodele de preprocesare."
      ],
      "metadata": {
        "id": "6kiV_U9JLobb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pentru analiza sentimentelor în tweet-uri, păstrarea emoticoanelor și eliminarea informațiilor neesențiale precum linkurile și mențiunile sunt pași critici, în timp ce normalizarea limbajului poate juca un rol cheie în îmbunătățirea preciziei analizei. Experimentarea și evaluarea impactului diferitelor metode de preprocesare asupra setului de date specific sunt esențiale pentru optimizarea procesului de analiză."
      ],
      "metadata": {
        "id": "trKyHqEMmTWH"
      }
    }
  ]
}